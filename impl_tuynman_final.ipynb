{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319f0b33",
   "metadata": {},
   "source": [
    "# Implementación Entorno de Ambulancias con Algoritmo *Tuynman*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ddf07",
   "metadata": {},
   "source": [
    "# CONFIGURACIÓN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4858d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports listos\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linprog, minimize\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "# Para reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Imports listos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5953ae1a",
   "metadata": {},
   "source": [
    "## Entorno: Sistema de Despacho de Ambulancias\n",
    "\n",
    "Replicamos el entorno establecido en SAVIA+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4553eb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiente inicializado: 9 estados, 3 acciones\n",
      "  Configuración: NA=2, NB=2\n",
      "  λ_total = 1.90/h, μ = 1.20/h\n",
      "  Λ (uniformización) = 6.70/h\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# AMBIENTE: Sistema de Despacho de Ambulancias\n",
    "# ============================================\n",
    "\n",
    "class AmbulanceEnvDFE:\n",
    "    \"\"\"\n",
    "    Entorno de despacho de ambulancias para DFE.\n",
    "    Idéntico al usado en SAVIA+ para comparación.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, NA=2, NB=2, zones=3, classes=2):\n",
    "        # Parámetros del sistema\n",
    "        self.NA = NA  # Ambulancias ALS\n",
    "        self.NB = NB  # Ambulancias BLS\n",
    "        self.Z = zones\n",
    "        self.C = classes\n",
    "        \n",
    "        # Espacio de estados: (sA, sB)\n",
    "        self.states = [(sa, sb) for sa in range(NA+1) for sb in range(NB+1)]\n",
    "        self.S = len(self.states)\n",
    "        self.state_to_idx = {s: i for i, s in enumerate(self.states)}\n",
    "        \n",
    "        # Espacio de acciones\n",
    "        self.actions = ['ALS', 'BLS', 'NOOP']\n",
    "        self.A = len(self.actions)\n",
    "        \n",
    "        # Tasas de llegada por zona y clase (por hora)\n",
    "        self.lambda_rates = {\n",
    "            (1, 'A'): 0.3, (1, 'B'): 0.6,\n",
    "            (2, 'A'): 0.2, (2, 'B'): 0.4,\n",
    "            (3, 'A'): 0.15, (3, 'B'): 0.25\n",
    "        }\n",
    "        self.lambda_total = sum(self.lambda_rates.values())  # 1.9 por hora\n",
    "        \n",
    "        # Tasa de servicio (por ambulancia por hora)\n",
    "        self.mu = 1.2\n",
    "        \n",
    "        # Tasa de uniformización\n",
    "        self.Lambda = self.lambda_total + self.mu * (NA + NB)\n",
    "        \n",
    "        # Probabilidades de señal imperfecta\n",
    "        self.omega_A = 0.85  # P(real A | señal A)\n",
    "        self.omega_B = 0.10  # P(real A | señal B)\n",
    "        \n",
    "        # Utilidades\n",
    "        self.U = {\n",
    "            ('A', 'ALS'): 1.00, ('A', 'BLS'): 0.85,\n",
    "            ('B', 'ALS'): 0.98, ('B', 'BLS'): 1.00\n",
    "        }\n",
    "        \n",
    "        # Parámetros de cobertura por zona\n",
    "        self.rho = {\n",
    "            ('ALS', 1): 0.80, ('ALS', 2): 0.60, ('ALS', 3): 0.50,\n",
    "            ('BLS', 1): 0.75, ('BLS', 2): 0.65, ('BLS', 3): 0.55\n",
    "        }\n",
    "        self.M_coverage = 5  # Saturación de cobertura\n",
    "        \n",
    "        print(f\"Ambiente inicializado: {self.S} estados, {self.A} acciones\")\n",
    "        print(f\"  Configuración: NA={NA}, NB={NB}\")\n",
    "        print(f\"  λ_total = {self.lambda_total:.2f}/h, μ = {self.mu:.2f}/h\")\n",
    "        print(f\"  Λ (uniformización) = {self.Lambda:.2f}/h\")\n",
    "    \n",
    "    def coverage_function(self, sq, q_type, zone):\n",
    "        \"\"\"\n",
    "        Función de cobertura\n",
    "        \"\"\"\n",
    "        rho_qz = self.rho[(q_type, zone)]\n",
    "        effective_units = min(sq, self.M_coverage)\n",
    "        return 1 - (1 - rho_qz) ** effective_units\n",
    "    \n",
    "    def expected_utility(self, signal, q_type):\n",
    "        \"\"\"\n",
    "        Utilidad esperada dado señal ω y tipo de ambulancia q.\n",
    "        \"\"\"\n",
    "        if signal == 'A':\n",
    "            return (self.omega_A * self.U[('A', q_type)] + \n",
    "                    (1 - self.omega_A) * self.U[('B', q_type)])\n",
    "        else:  # signal == 'B'\n",
    "            return (self.omega_B * self.U[('A', q_type)] + \n",
    "                    (1 - self.omega_B) * self.U[('B', q_type)])\n",
    "    \n",
    "    def reward(self, s, a, signal='A', zone=1):\n",
    "        \"\"\"\n",
    "        Recompensa por despachar ambulancia tipo a para paciente con señal en zona.\n",
    "        \"\"\"\n",
    "        sA, sB = s\n",
    "        \n",
    "        if a == 'NOOP':\n",
    "            return 0.0\n",
    "        \n",
    "        q_type = a  # 'ALS' o 'BLS'\n",
    "        sq = sA if q_type == 'ALS' else sB\n",
    "        \n",
    "        if sq == 0:  # No hay ambulancias disponibles\n",
    "            return 0.0\n",
    "        \n",
    "        util = self.expected_utility(signal, q_type)\n",
    "        coverage = self.coverage_function(sq, q_type, zone)\n",
    "        \n",
    "        return util * coverage\n",
    "    \n",
    "    def sample(self, s, a):\n",
    "        \"\"\"\n",
    "        sampleo del siguiente estado y recompensa dado estado s y acción a.\n",
    "        \"\"\"\n",
    "        sA, sB = s\n",
    "        \n",
    "        # Determinar tipo de evento según probabilidades de uniformización\n",
    "        rand = np.random.random()\n",
    "        cumulative = 0.0\n",
    "        \n",
    "        # Probabilidad de cada tipo de evento\n",
    "        event_probs = {}\n",
    "        \n",
    "        # ARRIVALs por zona y clase\n",
    "        for zone in range(1, self.Z + 1):\n",
    "            for cls in ['A', 'B']:\n",
    "                event_probs[('ARRIVAL', zone, cls)] = self.lambda_rates[(zone, cls)] / self.Lambda\n",
    "        \n",
    "        # Completaciones de servicio\n",
    "        event_probs[('ALS_DONE',)] = self.mu * (self.NA - sA) / self.Lambda\n",
    "        event_probs[('BLS_DONE',)] = self.mu * (self.NB - sB) / self.Lambda\n",
    "        \n",
    "        # NOOP \n",
    "        event_probs[('NOOP_EVENT',)] = 1 - sum(event_probs.values())\n",
    "        \n",
    "        # Samplear evento\n",
    "        for event, prob in event_probs.items():\n",
    "            cumulative += prob\n",
    "            if rand < cumulative:\n",
    "                break\n",
    "        \n",
    "        # Procesar evento\n",
    "        s_next = s\n",
    "        reward = 0.0\n",
    "        \n",
    "        if event[0] == 'ARRIVAL':\n",
    "            zone, cls = event[1], event[2]\n",
    "            signal = 'A' if cls == 'A' else 'B'\n",
    "            \n",
    "            # Calcular recompensa\n",
    "            reward = self.reward(s, a, signal, zone)\n",
    "            \n",
    "            # Aplicar acción\n",
    "            if a == 'ALS' and sA > 0:\n",
    "                s_next = (sA - 1, sB)\n",
    "            elif a == 'BLS' and sB > 0:\n",
    "                s_next = (sA, sB - 1)\n",
    "            # Si NOOP o no hay recursos, s_next = s\n",
    "            \n",
    "        elif event[0] == 'ALS_DONE':\n",
    "            s_next = (min(sA + 1, self.NA), sB)\n",
    "            \n",
    "        elif event[0] == 'BLS_DONE':\n",
    "            s_next = (sA, min(sB + 1, self.NB))\n",
    "        \n",
    "        # NOOP_EVENT no cambia estado ni da reward\n",
    "        \n",
    "        return s_next, reward\n",
    "    \n",
    "    def is_valid_action(self, s, a):\n",
    "        \"\"\"Verifica si la acción es válida en el estado s.\"\"\"\n",
    "        sA, sB = s\n",
    "        if a == 'ALS':\n",
    "            return sA > 0\n",
    "        elif a == 'BLS':\n",
    "            return sB > 0\n",
    "        else:  # NOOP\n",
    "            return True\n",
    "\n",
    "# Crear instancia del ambiente\n",
    "env = AmbulanceEnvDFE(NA=2, NB=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb21f56",
   "metadata": {},
   "source": [
    "# EVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfac62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones de confidence sets definidas\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CONFIDENCE SETS PARA TRANSICIONES\n",
    "# ============================================\n",
    "\n",
    "def compute_B(n, delta, S, A):\n",
    "    \"\"\"\n",
    "    Bound de confianza para ||p_hat - p||_1.\n",
    "    \n",
    "    B(n, δ) = sqrt(2*x(δ,n) / n)\n",
    "    donde x(δ,n) = log(SA/δ) + (S-1)*log(e*(1 + n/(S-1)))\n",
    "    \n",
    "    Args:\n",
    "        n: número de muestras\n",
    "        delta: nivel de confianza\n",
    "        \n",
    "    Returns:\n",
    "        B: bound tal que P(||p_hat - p||_1 <= B) >= 1 - δ\n",
    "    \"\"\"\n",
    "    \n",
    "    if n == 0:\n",
    "        return np.inf\n",
    "    \n",
    "    x = np.log(S * A / delta) + (S - 1) * np.log(np.e * (1 + n / (S - 1)))\n",
    "    B = np.sqrt(2 * x / n)\n",
    "    \n",
    "    return B\n",
    "\n",
    "def build_confidence_sets(p_hat_dict, n_dict, delta, S, A):\n",
    "    \"\"\"\n",
    "    Construye confidence sets P(s,a) para cada par estado-acción.\n",
    "    \n",
    "    P(s,a) = {p ∈ Σ_S : ||p - p_hat_{s,a}||_1 <= B(n_{s,a}, δ)}\n",
    "    \n",
    "    Args:\n",
    "        p_hat_dict: diccionario {(s,a): p_hat} con estimaciones empíricas\n",
    "        n_dict: diccionario {(s,a): n} con número de muestras\n",
    "        delta: nivel de confianza\n",
    "        \n",
    "    Returns:\n",
    "        P_sets: diccionario {(s,a): {'p_hat': array, 'radius': float}}\n",
    "    \"\"\"\n",
    "    P_sets = {}\n",
    "    \n",
    "    for (s, a), p_hat in p_hat_dict.items():\n",
    "        n = n_dict[(s, a)]\n",
    "        radius = compute_B(n, delta, S, A)\n",
    "        \n",
    "        P_sets[(s, a)] = {\n",
    "            'p_hat': p_hat,\n",
    "            'radius': radius\n",
    "        }\n",
    "    \n",
    "    return P_sets\n",
    "\n",
    "print(\"Funciones de confidence sets definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "505489e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extended Value Iteration (EVI-SSP) implementado\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EXTENDED VALUE ITERATION (EVI-SSP)\n",
    "# ============================================\n",
    "\n",
    "def optimistic_bellman(v, c, s, a, P_set, goal_state):\n",
    "    \"\"\"\n",
    "    Operador de Bellman extendido (optimista).\n",
    "    \"\"\"\n",
    "    if s == goal_state:\n",
    "        return 0.0, None\n",
    "    \n",
    "    p_hat = P_set['p_hat']\n",
    "    radius = P_set['radius']\n",
    "    \n",
    "    if radius >= 1.0:\n",
    "        return c + np.dot(p_hat, v), p_hat\n",
    "    \n",
    "    # Optimización: mover masa hacia estados con menor v\n",
    "    sorted_indices = np.argsort(v)\n",
    "    \n",
    "    p_opt = p_hat.copy()\n",
    "    budget = radius\n",
    "    \n",
    "    for i_low in sorted_indices:\n",
    "        if budget <= 0:\n",
    "            break\n",
    "        for i_high in reversed(sorted_indices):\n",
    "            if i_low >= i_high or budget <= 0:\n",
    "                break\n",
    "            \n",
    "            transfer = min(p_opt[i_high], budget / 2)\n",
    "            p_opt[i_high] -= transfer\n",
    "            p_opt[i_low] += transfer\n",
    "            budget -= 2 * transfer\n",
    "    \n",
    "    p_opt = np.maximum(p_opt, 0)\n",
    "    p_opt = p_opt / p_opt.sum()\n",
    "    \n",
    "    value = c + np.dot(p_opt, v)\n",
    "    \n",
    "    return value, p_opt\n",
    "\n",
    "\n",
    "def EVI_SSP(env, cost_func, goal_state, P_sets, mu_VI=0.5, max_iter=3000, verbose=False):\n",
    "    \"\"\"\n",
    "    Extended Value Iteration para SSP-MDP.\n",
    "    \"\"\"\n",
    "    S = env.S\n",
    "    A = env.A\n",
    "    \n",
    "    v = np.zeros(S)\n",
    "    v_prev = np.zeros(S)\n",
    "    \n",
    "    p_tilde = {}\n",
    "    pi_tilde = {}\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        v_prev = v.copy()\n",
    "        \n",
    "        for s_idx, s in enumerate(env.states):\n",
    "            if s_idx == goal_state:\n",
    "                v[s_idx] = 0.0\n",
    "                continue\n",
    "            \n",
    "            min_value = np.inf\n",
    "            best_a_idx = None\n",
    "            best_p = None\n",
    "            \n",
    "            for a_idx, a in enumerate(env.actions):\n",
    "                if not env.is_valid_action(s, a):\n",
    "                    continue\n",
    "                \n",
    "                c = cost_func(s, a)\n",
    "                P_set = P_sets.get((s, a), {'p_hat': np.ones(S)/S, 'radius': 1.0})\n",
    "                \n",
    "                value, p_opt = optimistic_bellman(v_prev, c, s_idx, a_idx, P_set, goal_state)\n",
    "                \n",
    "                if value < min_value:\n",
    "                    min_value = value\n",
    "                    best_a_idx = a_idx\n",
    "                    best_p = p_opt\n",
    "            \n",
    "            v[s_idx] = min_value\n",
    "            if best_a_idx is not None:\n",
    "                pi_tilde[s] = env.actions[best_a_idx]\n",
    "                p_tilde[(s, env.actions[best_a_idx])] = best_p\n",
    "        \n",
    "        # Convergencia\n",
    "        diff = np.max(np.abs(v - v_prev))\n",
    "        \n",
    "        if verbose and iteration % 100 == 0:\n",
    "            print(f\"      iter {iteration}: diff={diff:.6f}, max_v={np.max(v):.2f}\")\n",
    "        \n",
    "        if diff <= mu_VI:\n",
    "            if verbose:\n",
    "                print(f\"       Convergió en {iteration} iters\")\n",
    "            break\n",
    "    \n",
    "    if iteration >= max_iter - 1:\n",
    "        print(f\"     EVI-SSP NO convergió en {max_iter} iters (goal={goal_state})\")\n",
    "        print(f\"         diff final = {diff:.6f} > mu_VI = {mu_VI}\")\n",
    "    \n",
    "    return v, p_tilde, pi_tilde\n",
    "\n",
    "print(\" Extended Value Iteration (EVI-SSP) implementado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aba8a3",
   "metadata": {},
   "source": [
    "# Aplicación Algortimo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4f1e42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Algoritmo 2 (Diameter Estimation) implementado\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ALGORITHM 2: DIAMETER ESTIMATION\n",
    "# ============================================\n",
    "\n",
    "def compute_N(delta, eta, S, A):\n",
    "    \"\"\"\n",
    "    Número de muestras necesario para que B(N, δ) <= η.\n",
    "    \n",
    "    De la definición de B(n, δ):\n",
    "    B(n, δ) = sqrt(2*x(δ,n) / n) <= η\n",
    "    \n",
    "    Resolvemos para n (aproximación conservadora).\n",
    "    \"\"\"\n",
    "    # Bound conservador: x(δ, n) <= log(SA/δ) + (S-1)*log(e*(1 + n))\n",
    "    # Queremos: sqrt(2*[log(SA/δ) + (S-1)*log(e*(1+n))] / n) <= η\n",
    "    \n",
    "    # Aproximación: usar bound superior\n",
    "    # n >= C * (log(SA/δ) + S*log(n)) / η^2\n",
    "    \n",
    "    # Método iterativo\n",
    "    n = 1\n",
    "    for _ in range(100):  # Iteraciones de punto fijo\n",
    "        x_n = np.log(S * A / delta) + (S - 1) * np.log(np.e * (1 + n / (S - 1)))\n",
    "        n_new = int(np.ceil(2 * x_n / (eta ** 2)))\n",
    "        \n",
    "        if abs(n_new - n) < 10:\n",
    "            break\n",
    "        n = n_new\n",
    "    \n",
    "    return max(n, 1)\n",
    "\n",
    "\n",
    "def estimate_diameter(env, delta, epsilon=1.0, verbose=True):\n",
    "    \"\"\"\n",
    "    Algoritmo 2: Diameter Estimation.\n",
    "    \n",
    "    Estima D̂ tal que D ≤ D̂ ≤ 4D.\n",
    "    \n",
    "    Args:\n",
    "        env: ambiente\n",
    "        delta: nivel de confianza\n",
    "        epsilon: precisión relativa (default=1 para bound 4D)\n",
    "        verbose: imprimir progreso\n",
    "        \n",
    "    Returns:\n",
    "        D_hat: estimación del diámetro\n",
    "        samples_used: número total de samples\n",
    "        iteration_data: información de cada iteración del doubling trick\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ALGORITMO 2: DIAMETER ESTIMATION\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    W = 0.5\n",
    "    v_infty = 1.0\n",
    "    iteration = 0\n",
    "    samples_total = 0\n",
    "    iteration_data = []\n",
    "    \n",
    "    S = env.S\n",
    "    A = env.A\n",
    "    \n",
    "    while v_infty > W:\n",
    "        iteration += 1\n",
    "        W = 2 * W\n",
    "        eta = epsilon / (4 * W)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nIteración {iteration}:\")\n",
    "            print(f\"  W = {W:.3f}, η = {eta:.6f}\")\n",
    "        \n",
    "        # Número de muestras por (s, a)\n",
    "        N = compute_N(delta, eta, S, A)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  N (samples por (s,a)) = {N:,}\")\n",
    "        \n",
    "        # Samplear N veces cada (s, a)\n",
    "        p_hat_dict = {}\n",
    "        n_dict = {}\n",
    "        \n",
    "        for s in env.states:\n",
    "            for a in env.actions:\n",
    "                if not env.is_valid_action(s, a):\n",
    "                    continue\n",
    "                \n",
    "                # Contador de transiciones\n",
    "                transitions = np.zeros(S)\n",
    "                \n",
    "                for _ in range(N):\n",
    "                    s_next, _ = env.sample(s, a)\n",
    "                    s_next_idx = env.state_to_idx[s_next]\n",
    "                    transitions[s_next_idx] += 1\n",
    "                \n",
    "                # Estimación empírica\n",
    "                p_hat = transitions / N\n",
    "                p_hat_dict[(s, a)] = p_hat\n",
    "                n_dict[(s, a)] = N\n",
    "        \n",
    "        samples_iter = S * A * N  # Solo contamos los (s,a) válidos aprox\n",
    "        samples_total += samples_iter\n",
    "        \n",
    "        # Construir confidence sets\n",
    "        P_sets = build_confidence_sets(p_hat_dict, n_dict, delta, S=env.S, A=env.A)\n",
    "        \n",
    "        # Resolver SSP para cada par (s, s')\n",
    "        v_infty = 0.0\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Resolviendo {S} SSP-MDPs (uno por cada goal state)...\")\n",
    "        \n",
    "        for s_goal_idx, s_goal in enumerate(env.states):\n",
    "            # Definir función de costo: c(s, a) = 1 para todo (s, a)\n",
    "            def cost_func(s, a):\n",
    "                return 1.0\n",
    "            \n",
    "            # Resolver SSP con goal = s_goal\n",
    "            mu_VI = min(1.0, epsilon / 2)  # Precisión de VI. CAMBIO DE PARÁMETRO POR PROBLEMA DE CONVERGENCIA\n",
    "            # Verbose solo para los primeros 3 goals\n",
    "            verbose_evi = verbose and (s_goal_idx < 3)\n",
    "            if verbose_evi:\n",
    "                print(f\"    - Resolviendo SSP con goal={s_goal}\")\n",
    "            v_tilde, _, _ = EVI_SSP(env, cost_func, s_goal_idx, P_sets, mu_VI, verbose=verbose_evi)\n",
    "            \n",
    "            # Actualizar v_infty\n",
    "            for s_idx, s in enumerate(env.states):\n",
    "                if s_idx != s_goal_idx:\n",
    "                    v_infty = max(v_infty, v_tilde[s_idx])\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  ṽ_∞ = {v_infty:.6f}\")\n",
    "        \n",
    "        iteration_data.append({\n",
    "            'iteration': iteration,\n",
    "            'W': W,\n",
    "            'eta': eta,\n",
    "            'N': N,\n",
    "            'samples': samples_iter,\n",
    "            'v_infty': v_infty\n",
    "        })\n",
    "    \n",
    "    # Computar D_hat\n",
    "    D_hat = (1 + eta * v_infty / 2) * v_infty\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\" Convergencia alcanzada en iteración {iteration}\")\n",
    "        print(f\"  D̂ = {D_hat:.6f}\")\n",
    "        print(f\"  Total samples: {samples_total:,}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return D_hat, samples_total, iteration_data\n",
    "\n",
    "print(\" Algoritmo 2 (Diameter Estimation) implementado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f78492",
   "metadata": {},
   "source": [
    "# Aplicación Algoritmo 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998e701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Algoritmo 3 (Zurek-Chen) implementado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\g'\n",
      "/var/folders/v7/fy8jcw5s2y3b_6gklfsxb9fm0000gn/T/ipykernel_68055/2230755689.py:6: SyntaxWarning: invalid escape sequence '\\g'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ALGORITMO 3: ZUREK-CHEN \n",
    "# ============================================\n",
    "\n",
    "def value_iteration_discounted(env, gamma, p_hat_dict, r_tilde_dict, max_iter=10000, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Value Iteration para MDP descontado.\n",
    "    \n",
    "    Resuelve: V(s) = max_a [r(s,a) + \\gamma * Σ_s' p(s'|s,a) V(s')]\n",
    "    \n",
    "    Args:\n",
    "        env: ambiente\n",
    "        gamma: discount factor\n",
    "        p_hat_dict: transiciones empíricas {(s,a): p_array}\n",
    "        r_tilde_dict: recompensas perturbadas {(s,a): float}\n",
    "        max_iter: máximo número de iteraciones\n",
    "        tol: tolerancia para convergencia\n",
    "        \n",
    "    Returns:\n",
    "        V: función de valor óptima\n",
    "        Q: función Q óptima\n",
    "        pi: política óptima {s: a}\n",
    "    \"\"\"\n",
    "    S = env.S\n",
    "    A = env.A\n",
    "    \n",
    "    # Inicializar V = 0\n",
    "    V = np.zeros(S)\n",
    "    Q = np.zeros((S, A))\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        V_prev = V.copy()\n",
    "        \n",
    "        # Actualizar Q-values\n",
    "        for s_idx, s in enumerate(env.states):\n",
    "            for a_idx, a in enumerate(env.actions):\n",
    "                if not env.is_valid_action(s, a):\n",
    "                    Q[s_idx, a_idx] = -np.inf\n",
    "                    continue\n",
    "                \n",
    "                r = r_tilde_dict.get((s, a), 0.0)\n",
    "                p = p_hat_dict.get((s, a), np.ones(S) / S)\n",
    "                \n",
    "                Q[s_idx, a_idx] = r + gamma * np.dot(p, V_prev)\n",
    "        \n",
    "        # Actualizar V\n",
    "        V = np.max(Q, axis=1)\n",
    "        \n",
    "        # Convergencia\n",
    "        if np.max(np.abs(V - V_prev)) < tol:\n",
    "            break\n",
    "    \n",
    "    # Política óptima\n",
    "    pi = {}\n",
    "    for s_idx, s in enumerate(env.states):\n",
    "        best_a_idx = np.argmax(Q[s_idx, :])\n",
    "        pi[s] = env.actions[best_a_idx]\n",
    "    \n",
    "    return V, Q, pi\n",
    "\n",
    "\n",
    "def zurek_chen_policy(env, epsilon, H_upper, delta, verbose=True):\n",
    "    \"\"\"\n",
    "    Algoritmo 3: Zurek-Chen.\n",
    "    \n",
    "    Encuentra política ε-optimal usando upper bound H_upper en H.\n",
    "        \n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ALGORITMO: ZUREK-CHEN \")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Input: ε={epsilon}, H̄={H_upper:.3f}, δ={delta}\")\n",
    "    \n",
    "    S = env.S\n",
    "    A = env.A\n",
    "    \n",
    "    # Paso 1: Elegir discount factor\n",
    "    gamma = 1 - epsilon / (12 * H_upper)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nPaso 1: Discount factor\")\n",
    "        print(f\"  γ = 1 - ε/(12H̄) = {gamma:.8f}\")\n",
    "    \n",
    "    # Paso 2: Número de samples por (s, a)\n",
    "    n = int(np.ceil(\n",
    "        144 * H_upper / (epsilon ** 2) * np.log(12 * S * A / (delta * epsilon))\n",
    "    ))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nPaso 2: Uniform sampling\")\n",
    "        print(f\"  n (samples por (s,a)) = {n:,}\")\n",
    "        print(f\"  Total samples = {S * A * n:,}\")\n",
    "    \n",
    "    # Paso 3: Samplear n veces cada (s, a)\n",
    "    p_hat_dict = {}\n",
    "    r_samples_dict = {}\n",
    "    \n",
    "    for s in env.states:\n",
    "        for a in env.actions:\n",
    "            if not env.is_valid_action(s, a):\n",
    "                continue\n",
    "            \n",
    "            transitions = np.zeros(S)\n",
    "            rewards = []\n",
    "            \n",
    "            for _ in range(n):\n",
    "                s_next, r = env.sample(s, a)\n",
    "                s_next_idx = env.state_to_idx[s_next]\n",
    "                transitions[s_next_idx] += 1\n",
    "                rewards.append(r)\n",
    "            \n",
    "            p_hat_dict[(s, a)] = transitions / n\n",
    "            r_samples_dict[(s, a)] = np.mean(rewards)\n",
    "    \n",
    "    # Paso 4: Reward perturbation\n",
    "    r_tilde_dict = {}\n",
    "    np.random.seed(42)  # Para reproducibilidad\n",
    "    \n",
    "    for s in env.states:\n",
    "        for a in env.actions:\n",
    "            if not env.is_valid_action(s, a):\n",
    "                continue\n",
    "            \n",
    "            r_base = r_samples_dict.get((s, a), 0.0)\n",
    "            X_sa = np.random.uniform(0, epsilon / 72)\n",
    "            r_tilde_dict[(s, a)] = r_base + X_sa\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nPaso 3-4: Sampling & perturbation completados\")\n",
    "    \n",
    "    # Paso 5: Resolver MDP descontado empírico\n",
    "    if verbose:\n",
    "        print(f\"\\nPaso 5: Value Iteration (discounted MDP)\")\n",
    "    \n",
    "    V, Q, pi = value_iteration_discounted(env, gamma, p_hat_dict, r_tilde_dict)\n",
    "    \n",
    "    samples_used = S * A * n\n",
    "    \n",
    "    metadata = {\n",
    "        'gamma': gamma,\n",
    "        'n': n,\n",
    "        'V': V,\n",
    "        'Q': Q\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\" Política encontrada\")\n",
    "        print(f\"  Samples usados: {samples_used:,}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return pi, samples_used, metadata\n",
    "\n",
    "print(\" Algoritmo 3 (Zurek-Chen) implementado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06535f",
   "metadata": {},
   "source": [
    "# Aplicación Algoritmo Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0283f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Algoritmo DFE implementado\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ALGORITMO 1: DFE \n",
    "# ============================================\n",
    "\n",
    "def DFE(env, epsilon, delta, verbose=True):\n",
    "    \"\"\"\n",
    "    Diameter Free Exploration (Algorithm 1, página 6).\n",
    "    \n",
    "    Algoritmo principal que NO requiere conocimiento previo del MDP.\n",
    "    \n",
    "    Args:\n",
    "        env: ambiente\n",
    "        epsilon: precisión deseada (ε-optimality)\n",
    "        delta: nivel de confianza\n",
    "        verbose: imprimir progreso\n",
    "        \n",
    "    Returns:\n",
    "        pi: política ε-optimal\n",
    "        D_hat: estimación del diámetro\n",
    "        samples_total: número total de samples\n",
    "        results: diccionario con información detallada\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" \"*15 + \"DIAMETER FREE EXPLORATION (DFE)\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ============================================\n",
    "    # PASO 1: ESTIMAR DIÁMETRO\n",
    "    # ============================================\n",
    "    if verbose:\n",
    "        print(\"\\n[PASO 1/2] Estimando el diámetro ...\")\n",
    "        print(\"-\" * 70)\n",
    "    \n",
    "    D_hat, samples_diameter, diameter_data = estimate_diameter(\n",
    "        env, \n",
    "        delta=delta/2,  # Union bound\n",
    "        epsilon=1.0,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # ============================================\n",
    "    # PASO 2: ENCONTRAR POLÍTICA USANDO D̂\n",
    "    # ============================================\n",
    "    if verbose:\n",
    "        print(\"\\n[PASO 2/2] Encontrar una política óptima usando  D̂ como upper bound en H...\")\n",
    "        print(\"-\" * 70)\n",
    "    \n",
    "    pi, samples_policy, policy_metadata = zurek_chen_policy(\n",
    "        env,\n",
    "        epsilon=epsilon,\n",
    "        H_upper=D_hat,  # Usar D̂ como upper bound de H\n",
    "        delta=delta/2,  # Union bound\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # ============================================\n",
    "    # RESUMEN\n",
    "    # ============================================\n",
    "    samples_total = samples_diameter + samples_policy\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" \"*25 + \"DFE RESULTADOS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"  Diámetro estimado: D̂ = {D_hat:.6f}\")\n",
    "        print(f\"  Samples (diameter estimation): {samples_diameter:,}\")\n",
    "        print(f\"  Samples (policy identification): {samples_policy:,}\")\n",
    "        print(f\"  TOTAL SAMPLES: {samples_total:,}\")\n",
    "        print(f\"  Time elapsed: {elapsed_time:.2f} seconds\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    results = {\n",
    "        'D_hat': D_hat,\n",
    "        'samples_diameter': samples_diameter,\n",
    "        'samples_policy': samples_policy,\n",
    "        'samples_total': samples_total,\n",
    "        'diameter_data': diameter_data,\n",
    "        'policy_metadata': policy_metadata,\n",
    "        'elapsed_time': elapsed_time,\n",
    "        'epsilon': epsilon,\n",
    "        'delta': delta\n",
    "    }\n",
    "    \n",
    "    return pi, D_hat, samples_total, results\n",
    "\n",
    "print(\" Algoritmo DFE implementado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a6b9c",
   "metadata": {},
   "source": [
    "## Ejecución DFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3cc6ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración del experimento:\n",
      "  Estados: 9\n",
      "  Acciones: 3\n",
      "  ε = 0.5\n",
      "  δ = 0.3\n",
      "\n",
      "======================================================================\n",
      "Iniciando DFE...\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "               DIAMETER FREE EXPLORATION (DFE)\n",
      "======================================================================\n",
      "\n",
      "[PASO 1/2] Estimando el diámetro ...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "ALGORITMO 2: DIAMETER ESTIMATION\n",
      "============================================================\n",
      "\n",
      "Iteración 1:\n",
      "  W = 1.000, η = 0.250000\n",
      "  N (samples por (s,a)) = 1,804\n",
      "  Resolviendo 9 SSP-MDPs (uno por cada goal state)...\n",
      "    - Resolviendo SSP con goal=(0, 0)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 5 iters\n",
      "    - Resolviendo SSP con goal=(0, 1)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 5 iters\n",
      "    - Resolviendo SSP con goal=(0, 2)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 6 iters\n",
      "  ṽ_∞ = 9.622526\n",
      "\n",
      "Iteración 2:\n",
      "  W = 2.000, η = 0.125000\n",
      "  N (samples por (s,a)) = 8,866\n",
      "  Resolviendo 9 SSP-MDPs (uno por cada goal state)...\n",
      "    - Resolviendo SSP con goal=(0, 0)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 9 iters\n",
      "    - Resolviendo SSP con goal=(0, 1)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 7 iters\n",
      "    - Resolviendo SSP con goal=(0, 2)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 8 iters\n",
      "  ṽ_∞ = 10.328075\n",
      "\n",
      "Iteración 3:\n",
      "  W = 4.000, η = 0.062500\n",
      "  N (samples por (s,a)) = 41,818\n",
      "  Resolviendo 9 SSP-MDPs (uno por cada goal state)...\n",
      "    - Resolviendo SSP con goal=(0, 0)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 14 iters\n",
      "    - Resolviendo SSP con goal=(0, 1)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 10 iters\n",
      "    - Resolviendo SSP con goal=(0, 2)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 10 iters\n",
      "  ṽ_∞ = 11.261802\n",
      "\n",
      "Iteración 4:\n",
      "  W = 8.000, η = 0.031250\n",
      "  N (samples por (s,a)) = 192,289\n",
      "  Resolviendo 9 SSP-MDPs (uno por cada goal state)...\n",
      "    - Resolviendo SSP con goal=(0, 0)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 20 iters\n",
      "    - Resolviendo SSP con goal=(0, 1)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 12 iters\n",
      "    - Resolviendo SSP con goal=(0, 2)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 11 iters\n",
      "  ṽ_∞ = 15.765439\n",
      "\n",
      "Iteración 5:\n",
      "  W = 16.000, η = 0.015625\n",
      "  N (samples por (s,a)) = 867,925\n",
      "  Resolviendo 9 SSP-MDPs (uno por cada goal state)...\n",
      "    - Resolviendo SSP con goal=(0, 0)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 25 iters\n",
      "    - Resolviendo SSP con goal=(0, 1)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 13 iters\n",
      "    - Resolviendo SSP con goal=(0, 2)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 11 iters\n",
      "  ṽ_∞ = 19.582555\n",
      "\n",
      "Iteración 6:\n",
      "  W = 32.000, η = 0.007812\n",
      "  N (samples por (s,a)) = 3,863,129\n",
      "  Resolviendo 9 SSP-MDPs (uno por cada goal state)...\n",
      "    - Resolviendo SSP con goal=(0, 0)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 28 iters\n",
      "    - Resolviendo SSP con goal=(0, 1)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 14 iters\n",
      "    - Resolviendo SSP con goal=(0, 2)\n",
      "      iter 0: diff=1.000000, max_v=1.00\n",
      "       Convergió en 12 iters\n",
      "  ṽ_∞ = 22.043401\n",
      "\n",
      "============================================================\n",
      " Convergencia alcanzada en iteración 6\n",
      "  D̂ = 23.941493\n",
      "  Total samples: 134,347,437\n",
      "============================================================\n",
      "\n",
      "\n",
      "[PASO 2/2] Encontrar una política óptima usando  D̂ como upper bound en H...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "ALGORITMO: ZUREK-CHEN \n",
      "============================================================\n",
      "Input: ε=0.5, H̄=23.941, δ=0.15\n",
      "\n",
      "Paso 1: Discount factor\n",
      "  γ = 1 - ε/(12H̄) = 0.99825965\n",
      "\n",
      "Paso 2: Uniform sampling\n",
      "  n (samples por (s,a)) = 115,439\n",
      "  Total samples = 3,116,853\n",
      "\n",
      "Paso 3-4: Sampling & perturbation completados\n",
      "\n",
      "Paso 5: Value Iteration (discounted MDP)\n",
      "\n",
      "============================================================\n",
      " Política encontrada\n",
      "  Samples usados: 3,116,853\n",
      "============================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "                         DFE RESULTADOS\n",
      "======================================================================\n",
      "  Diámetro estimado: D̂ = 23.941493\n",
      "  Samples (diameter estimation): 134,347,437\n",
      "  Samples (policy identification): 3,116,853\n",
      "  TOTAL SAMPLES: 137,464,290\n",
      "  Time elapsed: 216.14 seconds\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EJECUCIÓN DEL DFE\n",
    "# ============================================\n",
    "\n",
    "# Parámetros\n",
    "epsilon = 0.5\n",
    "delta = 0.3\n",
    "\n",
    "print(\"Configuración del experimento:\")\n",
    "print(f\"  Estados: {env.S}\")\n",
    "print(f\"  Acciones: {env.A}\")\n",
    "print(f\"  ε = {epsilon}\")\n",
    "print(f\"  δ = {delta}\")\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Iniciando DFE...\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Ejecutar DFE\n",
    "pi_dfe, D_hat, samples_dfe, results_dfe = DFE(env, epsilon, delta, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e889bb",
   "metadata": {},
   "source": [
    "## Visualización de la Política DFE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85227b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " POLÍTICA ÓPTIMA DFE COMPLETA:\n",
      "======================================================================\n",
      "\n",
      "        Ambulancias BLS libres\n",
      "        0    1    2  \n",
      "    +---------------\n",
      "     0 | NOOP BLS  BLS \n",
      " ALS 1 | ALS  ALS  BLS \n",
      "     2 | ALS  ALS  ALS \n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# VISUALIZACIÓN: POLÍTICA DFE OUTPUT\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Crear matriz de política desde el diccionario\n",
    "NA = env.NA\n",
    "NB = env.NB\n",
    "\n",
    "policy_grid_dfe = np.empty((NA + 1, NB + 1), dtype=object)\n",
    "\n",
    "for s in env.states:\n",
    "    sA, sB = s\n",
    "    action = pi_dfe[s]\n",
    "    \n",
    "    # Abreviar acciones\n",
    "    if action == 'ALS':\n",
    "        symbol = 'ALS'\n",
    "    elif action == 'BLS':\n",
    "        symbol = 'BLS'\n",
    "    else:\n",
    "        symbol = 'NOOP'\n",
    "    \n",
    "    policy_grid_dfe[sA, sB] = symbol\n",
    "\n",
    "# Imprimir tabla bonita\n",
    "print(\"\\n POLÍTICA ÓPTIMA DFE COMPLETA:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n        Ambulancias BLS libres\")\n",
    "print(\"      \", end=\"\")\n",
    "for sB in range(NB + 1):\n",
    "    print(f\"  {sB}  \", end=\"\")\n",
    "print(\"\\n    +\" + \"-----\" * (NB + 1))\n",
    "\n",
    "for sA in range(NA + 1):\n",
    "    # Label \"ALS\" solo en la fila del medio\n",
    "    if sA == NA // 2:\n",
    "        print(f\" ALS \", end=\"\")\n",
    "    else:\n",
    "        print(f\"     \", end=\"\")\n",
    "    \n",
    "    print(f\"{sA} |\", end=\"\")\n",
    "    \n",
    "    for sB in range(NB + 1):\n",
    "        print(f\" {policy_grid_dfe[sA, sB]:^4}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66b59d4",
   "metadata": {},
   "source": [
    "## AVG REWARD del algoritmo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03031d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando política DFE mediante simulación...\n",
      "\n",
      "======================================================================\n",
      "                    Simulación\n",
      "======================================================================\n",
      "Pasos simulados: 100,000\n",
      "\n",
      "Average Reward:\n",
      "  Por paso (uniformizado):  0.208234\n",
      "  Por hora:                1.395171\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# AVERAGE REWARD DE POLÍTICA DFE\n",
    "# ============================================\n",
    "\n",
    "def simulate_policy(env, pi, num_steps=100000, verbose=True):\n",
    "    \"\"\"\n",
    "    Simula una política en el ambiente para estimar el average reward.\n",
    "    \n",
    "    Args:\n",
    "        env: ambiente\n",
    "        pi: política {s: a}\n",
    "        num_steps: número de pasos de simulación\n",
    "        verbose: imprimir resultados\n",
    "        \n",
    "    Returns:\n",
    "        metrics: diccionario con métricas de desempeño\n",
    "    \"\"\"\n",
    "    # Estado inicial (recursos máximos)\n",
    "    s = (env.NA, env.NB)\n",
    "    \n",
    "    # Contadores\n",
    "    total_reward = 0.0\n",
    "    event_counts = defaultdict(int)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Obtener acción de la política\n",
    "        a = pi[s]\n",
    "        \n",
    "        # Samplear transición\n",
    "        s_next, reward = env.sample(s, a)\n",
    "        \n",
    "        # Acumular reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Registrar tipo de evento (para diagnóstico)\n",
    "        # Inferir evento basado en cambio de estado\n",
    "        sA, sB = s\n",
    "        sA_next, sB_next = s_next\n",
    "        \n",
    "        if sA_next < sA:  # ALS fue despachado\n",
    "            event_counts['ALS_DISPATCH'] += 1\n",
    "        elif sB_next < sB:  # BLS fue despachado\n",
    "            event_counts['BLS_DISPATCH'] += 1\n",
    "        elif sA_next > sA:  # ALS regresó\n",
    "            event_counts['ALS_DONE'] += 1\n",
    "        elif sB_next > sB:  # BLS regresó\n",
    "            event_counts['BLS_DONE'] += 1\n",
    "        else:  # Estado no cambió\n",
    "            event_counts['NOOP_OR_ARRIVAL_LOST'] += 1\n",
    "        \n",
    "        s = s_next\n",
    "    \n",
    "    # Calcular métricas\n",
    "    avg_reward_per_step = total_reward / num_steps\n",
    "    avg_reward_per_hour = avg_reward_per_step * env.Lambda\n",
    "    \n",
    "    # Estimar tasa de arrivals atendidos\n",
    "    arrivals_served = event_counts['ALS_DISPATCH'] + event_counts['BLS_DISPATCH']\n",
    "    arrival_rate_observed = arrivals_served / num_steps * env.Lambda\n",
    "    \n",
    "    # Reward por ARRIVAL\n",
    "    if arrivals_served > 0:\n",
    "        avg_reward_per_arrival = total_reward / arrivals_served\n",
    "    else:\n",
    "        avg_reward_per_arrival = 0.0\n",
    "    \n",
    "    metrics = {\n",
    "        'avg_reward_per_step': avg_reward_per_step,\n",
    "        'avg_reward_per_hour': avg_reward_per_hour,\n",
    "        'avg_reward_per_arrival': avg_reward_per_arrival,\n",
    "        'total_reward': total_reward,\n",
    "        'arrivals_served': arrivals_served,\n",
    "        'arrival_rate_observed': arrival_rate_observed,\n",
    "        'event_counts': dict(event_counts),\n",
    "        'num_steps': num_steps\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" \"*20 + \"Simulación\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Pasos simulados: {num_steps:,}\")\n",
    "        print(f\"\\nAverage Reward:\")\n",
    "        print(f\"  Por paso (uniformizado):  {avg_reward_per_step:.6f}\")\n",
    "        print(f\"  Por hora:                {avg_reward_per_hour:.6f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluar política DFE\n",
    "print(\"Evaluando política DFE mediante simulación...\")\n",
    "metrics_dfe = simulate_policy(env, pi_dfe, num_steps=100000, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
